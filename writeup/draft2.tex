\documentclass{article}
\usepackage{changepage}
\usepackage{noah}


\usepackage[font=small,labelfont=bf]{caption} 
%% Stuff for inserting code into document
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\author{Noah Jeffrey Blair}
\title{Vibrations and Heat Diffusion on the Unit Disc}
\begin{document}
\maketitle
\begin{abstract}
  The method of eigenfunction expansion is developed and demonstrated for solutions to the Helmholtz equation on the unit disc. Through this method, Greens' functions for the wave and heat equation are generated and particular solutions are analyzed, to show the variety of dynamics possible within the realm of partial differential equations.
\end{abstract}
\tableofcontents
\newpage
\section{Introduction}
Partial differential equations (PDEs) are a natural extension to  ordinary differential equations, in that they describe systems that can vary with respect to multiple variables, traditionally position and time. Two fundamental PDEs are the heat and wave equations. Both of these involve the Laplace operator, defined in the following way:
\begin{equation}
  \lapl f := \dive(\grad f)
\end{equation}
Note, that it is sometimes written as $\nabla^2 f$. Due to its nature as being the divergence of the gradient of some function, it is a natural generalization to the second derivative with respect to a position variable, and can be written in the following way:
\begin{equation}
  \lapl f = \pder{^2 f}{x^2}+\pder{^2 f}{y^2}= \pder{^2 f}{r^2}+\frac{1}{r}\pder{f}{r}+\frac{1}{r^2}\pder{^2 f}{\phi^2}
\end{equation}
where $(x,y)$ represent a point in Cartesian coordinates and $(r,\phi)$ represents a point in polar coordinates. Solutions to Laplace's equation $\lapl u = 0$ are known as harmonic functions, and are of particular interest in both pure and applied mathematics.\\
This operator is used regularly in the description of many dynamical systems, including wave propagation, heat diffusion, and many other PDEs.\\
The wave equation, as the name suggests, describes how waves propagate through a given domain. Consider a string with fixed ends that is allowed to vibrate. The motion that we are interested in is transverse motion, so we consider the forces in a direction normal to the line. Newton's second law tells us the following for the dynamics of a small bit of mass on a string:
\begin{equation}
    F_{net}=dm\cdot a=\lambda dx \cdot a
\end{equation}
where $\lambda$ is the linear mass density, and $a$ is the vertical acceleration of the string. If we raise part of the string near this point so that the angle the string makes between these two points is $\theta$, then the force is:
\begin{equation}
    F_{net}=T \sin(\theta + d\theta)-T \sin(\theta)=T\cos(\theta)d\theta)
\end{equation}
where $T$ is the tension in the rope. If we assume small enough displacements, we can approximated $\cos\theta\approx 1$ and write $d\theta= (\partial\theta/\partial x) dx$. Since we are assuming small displacements, we can say that the angle $\theta$ is the slope of the string. We will write the displacement of the string as $u=u(x,t)$, and say that this function must satisfy the following equation:
\begin{equation}\pder{^2 u}{t^2}= \frac{T}{\lambda}\pder{^2 u}{x^2}=c^2 \pder{^2 u}{x^2}\end{equation}
We are introducing the constant $c$ which is the speed of propagation on the string. This can be absorbed into our time variable, by redefining our unit of time. For higher spacial dimensions, the second partial derivative with respect to $x$ is replaced with the Laplacian, so the general wave equation is:
\begin{equation}
  \boxed{
    \lapl u =\pder{^2 u}{t^2}
  }
\end{equation}
We see that when the vibrating object is not accelerating, the Laplacian is zero, which means that the steady state behavior of the wave equation is a harmonic function. As we will see later, solutions to the wave equation oscillate about harmonic functions.\\
The heat equation has a similar form as the wave equation, but with very different dynamics. The general assumption behind the heat equation is conservation of energy. This conservation law is formulated in terms of a continuity equation:
\begin{equation}
  \pder{u}{t}+\dive \vect{J}=0
\end{equation}
Here $u$ is the energy density, and $\vect{J}$ is the energy flux. This equation states that if energy is flowing into a region, then the energy density must be increasing. In order to find the energy flux, we use Newton's Law of cooling which states that the energy flow is proportional to the temperature difference between two regions. This can be formulated in the following way:
\begin{equation}
  \vect{J}=-\sigma\grad u
\end{equation}
where $\sigma$ is the thermal conductivity. The reason for using the energy gradient instead of the temperature gradient in this equation is that temperature is just a useful method for defining the energy. Using this, we may write the heat equation as:
\begin{equation}
  \boxed{
    \lapl u = \pder{u}{t}
  }
\end{equation}
Here we have absorbed the thermal conductivity into our definition of time. The interesting part of this equation is that it can be used to describe diffusive processes as well. In fact it can be used to describe many systems that do no violate the first and second law of thermodynamics. The first of these states energy conservation, and the second states that the entropy of a system must increase. This second law is one of the few that does depend on the direction of time. This is why the heat equation is not symmetric in the spatial and temporal variables, when the wave equation is.\\
This system has harmonic solutions whenever the system is not changing in time. Therefore, we see that in time, the solution tends towards a harmonic solution. It is for this reason that solutions to Laplace's equations describe steady state temperature distributions.\\
In order to analyze the different types of solutions these equations provide, as well as some general methods for solving PDEs, we will solve both of these equations on the unit disc $B^2$ with the boundary condition that the function must vanish at the boundary. Similar to the case for ODEs, we must also provide initial conditions. Since the heat equation is first order in time, we only need to know the initial heat distribution $u(r,\phi,0)=u_o(r,\phi)$. The wave equation is second order in time, so we must supply both the initial displacement of the circular membrane, $u(r,\phi,0)=u_o(r,\phi)$, and the initial velocity of $\der{t} u(r,\phi,0)=\dot{u}_o(r,\phi)$. Together with the boundary data, these conditions uniquely define a solution to the differential equation.
\section{Separation of Variables}
To begin solving these equations, we will employ a method known as separation of variables. In this we assume that the solution can be written as the product of single-variable functions. Since these are both functions of position and time, we assume a solution $u(r,\phi,t)=\psi(r,\phi)\chi(t)$. If we insert this into the heat equation, and divide by $u(r,\phi,t)$ we obtain the following:
\begin{equation}
  \frac{\lapl \psi}{\psi} = \frac{1}{\chi(t)}\dder{\chi}{t}
\end{equation}
The left-hand side is just a function of the spacial variables, while the right-hand side is just a function of time. This means that the two sides are independent of each other, and therefore must be equal to a constant. For reasons that will be clear later on, I will use $-k^2$ as the constant. This leaves us with two differential equations:
\begin{equation}
  \begin{matrix}
    \lapl\psi+k^2\psi=0\\
    \dot{\chi}=-k^2 \chi
  \end{matrix}
\end{equation}
We can easily solve the time equation to obtain:
\begin{equation}
  \chi_{\mathrm{heat}}(t)=\exp(-k^2 t)
\end{equation}
Any constants of integration may be absorbed into $\psi(r,\phi)$. If we follow this same procedure for the wave equation, we are left with the two separated differential equations:
\begin{equation}
  \begin{matrix}
    \lapl\psi+k^2\psi=0\\
    \ddot{\chi}=-k^2 \chi
  \end{matrix}
\end{equation}
Again the time component can be easily solved, to give the following solution:
\begin{equation}
  \chi_{\mathrm{wave}}=A\cos(kt)+B\sin(kt)
\end{equation}
It is important to notice that for both of these equations, the spacial part is the same. Given that we are using the same boundary conditions for both equations, solving this equation will provide us with solutions to both the wave and heat equation.
\section{The Helmholtz Equation}
The above differential equation:
\begin{equation}
  \lapl \psi+k^2\psi=0
\end{equation}
is known as the Helmholtz equation, and it describes the eigenvalues and eigenfunctions of the Laplacian on the unit disc. Recall that an eigenvalue equation is one where:
\begin{equation}
  \mc{L}v = \lambda v
\end{equation}
where $\mc{L}$ is a linear operator, $v$ is an eigenvector, and $\lambda$ is an eigenvalue. In the context of differential operators, the eigenvectors are referred to as eigenfunctions.\\
Given our boundary data, the eigenfunctions will be ones that satisfy the following equations:
\begin{equation}
  \boxed{
    \begin{matrix}
      \lapl \psi+k^2\psi=0 \\ 
      \psi(1,\phi) = 0
    \end{matrix}
  }
\end{equation}
In order to solve this equation, we again separate the variables, now assuming that $\psi(r,\phi)=f(r)g(\phi)$. If we insert this into the Helmholtz equation and divide by $\psi$ we obtain:
\begin{equation}
  \frac{1}{f(r)}\left[r^2 \dder{^2 f}{r^2} + r \dder{f}{r}+k^2 r^2\right] = -\frac{1}{g(\phi)}\dder{^2 g}{\phi^2} = n^2
\end{equation}
Again the left-hand side is a function of $r$ only, and the right hand side is a function of $\phi$ only. For reasons that will be obvious in later sections, we have called the separation constant $n^2$. The condition on the radial equation is that $f(1) = 0$. The condition on the angular equation is less obvious though. In order to get physical results, as well as to make sense of partial derivatives, we require our functions to be twice differentiable. This implies that our angular equation must at the very least be continuous, and therefore we have the implicit condition that $g(0)=g(2\pi)$.
\section{Inner Products and Orthogonal Basis}
In order to better describe the solutions to the above equations, we will review some concepts from linear algebra. Recall that the inner product is a bilinear form that maps two elements from the vector space to the real or complex numbers. Though it has other properties that the map must satisfy, it is sufficient for our purposes to note that it is a generalization of the scalar product in $\bb{R}^3$ and can be used to define orthogonality. Two vectors are orthogonal if their inner product is zero:
\begin{equation}
  (u,v)=0
\end{equation}
The other important concept is that of a change of basis. A basis is any set of vectors $\{\bv{i}\}$ such that any vector in the vector space may be written as a unique linear combination of the basis vectors:
\begin{equation}
  v = \sum_{i} v_i \bv{i}
\end{equation}
An orthogonal basis is one where each of the basis vectors satisfies:
\begin{equation}
  (\bv{i},\bv{j})= A_i \delta_{ij}
\end{equation}
where $A_i$ is the length of a given basis vector, and $\delta_{ij}$ is the Kronecker delta defined as:
\begin{equation}
  \delta_{ij} = \syst{
    1; i = j \\
    0; i \neq j
  }
\end{equation}
The inner product can then be used to find the components of a vector in a given basis:
\begin{equation}
  (\bv{i},v)=\sum_{j} v_j (\bv{i},\bv{j})\rightarrow v_i = \frac{(\bv{i},v)}{\left|\left| \bv{i}\right|\right|^2}
\end{equation}
These results are not limited to finite dimensional vector spaces, and will be used frequently for the results in later sections. We will regularly be working with the vector space $L^2(\Omega)$, which is the set of functions that are square integrable on some domain $\Omega$. The most common inner product for these function-spaces is defined in the following way:
\begin{equation}
  \boxed{
    (f,g) = \int_\Omega f^\star(x) g(x)dx
  }
\end{equation}
where $\star$ is denotes a complex conjugation. 
\section{Fourier Series}
The angular equation for the Helmholtz equation is:
\begin{equation}
  g''+n^2 g=0
\end{equation}
This has the solution:
\begin{equation}
  g(\phi) = \exp(i n \phi)
\end{equation}
we can account for the two linearly independent solutions by allowing $n$ to run from positive to negative values. In order for $g$ to be a continuous function, we see that $e^{2\pi i n} = 1$ which requires that $n\in\bb{Z}$.\\
In order for us to match the initial conditions of the heat or wave equation, we will need to sum different solutions to the Helmholtz equation in order to construct the proper solution. For the time being, let us consider representing a function in the following way:
\begin{equation}
  F(\phi) = \sum_{n = -\infty}^\infty c_n e^{in\phi}
\end{equation}
This is known as a Fourier series, and is the method of rewriting a function on the unit circle $S^1$ as sum of complex exponentials. In the language of linear algebra, these complex exponentials are a basis for $L^2(S^1,d\phi)$ and the coefficients $c_n$ are the components of the function. The inner product for this space is:
\begin{equation}
  (f(\phi),g(\phi)) = \int_0^{2\pi} f^\star(\phi)g(\phi) d\phi
\end{equation}
If we integrate two of our basis functions, we see that:
\begin{equation}
  \int_0^{2\pi} e^{-i n' \phi} e^{i n \phi} d\phi = \frac{1}{i(n-n')}\left(e^{2\pi i (n-n')} - 1\right)
\end{equation}
If $n\neq n'$ then $n-n'$ is some non-zero integer, and the integral is zero. This means that our functions are orthogonal. If they are the same function, then we have:
\begin{equation}
  \left(e^{in\phi}, e^{i n\phi} \right) = \norm{e^{in\phi}}^2 = 2\pi
\end{equation}
This gives the orthogonality relationship for our complex exponentials:
\begin{equation}
  \boxed{
  \left(e^{in\phi}, e^{i m\phi} \right) = 2\pi \delta_{nm}
  }
\end{equation}
If we take the inner product of our basis function with the function $F(\phi)$, we obtain:
\begin{equation}
  \left(e^{i n \phi},F(\phi)\right) = \sum_{m = -\infty}^\infty c_m \left(e^{in\phi}, e^{i m\phi} \right) = 2\pi c_n
\end{equation}
This can be used to find the coefficients in terms of an integral for the function:
\begin{equation}
  \boxed{
    c_n = \frac{1}{2\pi}\int_0^{2\pi} F(\phi) e^{-in\phi} d\phi
  }
\end{equation}
These results will be used later to match the initial conditions.
\section{Fourier-Bessel Series}
The radial part of the Helmholtz equation is:
\begin{equation}
  r^2 f''+ rf' + (k^2 r^2 - n^2) f = 0
\end{equation}
This is known as Bessel's differential equation, and has the following two solutions:
\begin{equation}
  f(r) = c_1 J_n(k r) + c_2 Y_n (k r)
\end{equation}
where $J_n$ is the $n$-th Bessel function of the first kind, or simply the $n$-th Bessel function, and where $Y_n$ is the $n$-th Bessel function of the second kind, or simply the $n$-th Neumann function. Neumann functions have poles at the origin, so in order for solutions to be physical, we require $c_2 = 0$. For simplicity, we will let $c_1 = 2$. Our boundary condition that $f(1)= 0$ determines our boundary conditions. The eigenvalues of the Laplacian must satisfy the following:
\begin{equation}
  \boxed{
    J_n(k) = 0
  }
\end{equation}
It is known that the integer order Bessel functions have an infinite number of zeros, so we will let $k = \alpha_{nm}$ denote the $m$-th zero of the $n$-th order Bessel function. There is no closed form solution for these zeros, but they can be calculated numerically. In Mathematica, the code is $\alpha_{nm} = \texttt{BesselJZero[n,m]}$.\\
Similar to the complex exponentials, we may expand functions which are elements of $L^2([0,1],rdr)$ in an infinite series of Bessel functions, in a method known as a Fourier-Bessel series.
\begin{equation}
  F(r) = \sum_{m = 1}^{\infty} \sum_{n = -\infty}^\infty c_{nm} J_n(\alpha_{nm} r)
\end{equation}
The factor of $r$ in the definition of the vector space denotes the metric function for the inner product. This is here because we are integrating the radial part of functions defined on the unit disc. Therefore the inner product for this vector space is:
\begin{equation}
  (f(r),g(r)) = \int_0^1 f^\star(r)g(r)r dr
\end{equation}
In general, an inner product of this nature may be written as:
\begin{equation}
  (f,g) = \int_\Omega f^\star (x) g(x) w(x)dx
\end{equation}
where $w(x)$ is a metric function and plays a similar role to the metric tensor in tensor analysis.\\
Using this inner product, it can be shown that the Bessel functions satisfy the following orthogonality relationship:
\begin{equation}
  \boxed{
    \left(J_{n'}(\alpha_{n'm'}r),J_{n}(\alpha_{nm}r)\right) =\delta_{nn'}\delta_{mm'}\frac{1}{2}(J_{n+1}(\alpha_{nm}))^2
  }
\end{equation}
Using this relationship, we can write the expansion coefficients for the Bessel-Fourier series as:
\begin{equation}
  \boxed{
    c_{nm} = \frac{2}{J_{n+1}(\alpha_{nm})^2}\int_0^1 F(r)J_n(\alpha_{nm}r) rdr
  }
\end{equation}
\section{The Normal Modes on a Disc}
If we combine the radial and angular part of the solution to the Helmholtz equation, we obtain a set of functions that satisfy the Helmholtz equation, the boundary data, and form a basis for $L^2(B^2,rdrd\phi)$. 
\begin{equation}
  \boxed{
    \psi_{nm}(r,\phi) = J_n ( \alpha_{nm} r) e^{i n \phi}
  }
\end{equation}
We refer to these basis functions as the normal modes for this solution, for a solution to the wave equation whose spacial part is one of the functions will vibrate at a single frequency.\\
The inner product for this space is defined as a combination for the inner products for the radial and angular components,
\begin{equation}
  \boxed{
    \left(f(r,\phi),g(r,\phi)\right) = \int_0 ^1 dr \int_0 ^{2\pi} dr f^\star(r,\phi) g(r,\phi) r
  }
\end{equation}
Our normal modes will therefore satisfy the following orthogonality relationship:
\begin{equation}
  (\psi_{n'm'},\psi_{nm}) = \delta_{n'n}\delta_{m'm} \pi J_{n+1}( \alpha_{nm } )^2
\end{equation}
Our goal now is to use a method similar to the method for Fourier series and Fourier-Bessel series to expand any function into an infinite series of these normal modes. This is known as an eigenfunction-expansion, and is valid for expanding solutions to the equation:
\begin{equation}
  \mc{L} y_n = \lambda_n y_n
\end{equation}
where $\mc{L}$ is an operator that satisfies the following for the given inner product:
\begin{equation}
  (\mc{L} f,g) = (f,\mc{L} g)
\end{equation}
In the case of our normal modes, we hope to write any function $F:B^2\rightarrow\bb{C}$ as:
\begin{equation}
  \boxed{
    F = \sum_{m = 1}^\infty\sum_{n = -\infty}^\infty c_{nm} \psi_{nm} = \sum_{nm} c_{nm}\psi_{nm}
  }
\end{equation}
If we take the inner product between one of our basis functions, and solve for $c_{nm}$ we obtain the following equation:
\begin{equation}
  \boxed{
    c_{nm} = \frac{1}{\pi J_{n+1}(\alpha_{nm})^2}\int_0^{2\pi} d\phi\int_0^1 dr F(r,\phi) r e^{- i n \phi}J_n(\alpha_{nm} r)
  }
\end{equation}
\section{Matching Initial Conditions}
\section{A Particular Solution}

\newpage
\section{References}
Asmar, N. H. (2002). \textit{Applied complex analysis with partial differential equations}. New Jersey: Prentice-Hall Inc.\\
\begin{adjustwidth}{2 cm}{}
This book was used to understand differential equations in polar and spherical coordinate systems, as well as certain properties of Bessel functions.\\
\end{adjustwidth}
Farlow, S. J. (2016). \textit{Partial differential equations for scientists and engineers}. New York: Dover Publications, Inc.\\
\begin{adjustwidth}{2 cm}{}
This book was used to understand the basic types of partial differential equations, as well as some methods for solving them in one dimension. It also introduced the physical interpretation of these equations.\\
\end{adjustwidth}
Fetter, A. L., \& Walecka, J. D. (2003). \textit{Theoretical mechanics of particles and continua}. Mineola, NY: Dover Publications. \\
\begin{adjustwidth}{2 cm}{}
This book was used to understand the physical meaning behind the wave and heat equation, as well as some theorems from Sturm-Liouville theory.\\
\end{adjustwidth}
Tolstov, G. P. (2009). \textit{Fourier series}. New York: Dover Publications, Inc.\\
\begin{adjustwidth}{2 cm}{}
This book was used to understand basic properties of Fourier Series and Fourier-Bessel Series, as well as general methods of eigenfunction expansions.
\end{adjustwidth}
\end{document}